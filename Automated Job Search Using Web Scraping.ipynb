{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33862bc5",
   "metadata": {},
   "source": [
    "# Automated Job Search Using Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71bb87",
   "metadata": {},
   "source": [
    "### Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f59946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748ef4e",
   "metadata": {},
   "source": [
    "### Now, we will scrap data from timesjobs.com website and save the data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95345e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Data from\n",
      "https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=\n",
      ".\n",
      "25 new jobs updated in Jobs.csv.\n"
     ]
    }
   ],
   "source": [
    "urls = \"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=\"\n",
    "\n",
    "def findJobs():\n",
    "    count=0\n",
    "    html_text = requests.get(urls).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "    jobs_data = []  # List to store job data\n",
    "\n",
    "    for index, job in enumerate(jobs):\n",
    "        location = job.find('ul', class_=\"top-jd-dtl clearfix\").span.text\n",
    "        skills = job.find('ul', class_=\"list-job-dtl clearfix\").span.text\n",
    "        company = job.find('h3', class_=\"joblist-comp-name\").text\n",
    "        link = job.find('header', class_=\"clearfix\").h2.a['href']\n",
    "        \n",
    "        count+=1\n",
    "        jobs_data.append({\n",
    "            'Company': company.strip().replace('(More Jobs)',''),\n",
    "            'Location': location.strip(),\n",
    "            'Skills': skills.strip(),\n",
    "            'Link': link\n",
    "        })\n",
    "        \n",
    "        \n",
    "   # Save job data to a CSV file\n",
    "    if not os.path.exists('posts'):\n",
    "        os.makedirs('posts')\n",
    "    df = pd.DataFrame(jobs_data)\n",
    "    df.to_csv('posts/Jobs.csv', index=False)\n",
    "\n",
    "    return count\n",
    "\n",
    "print(f'Fetching Data from\\n{urls}\\n.')\n",
    "x = findJobs()\n",
    "print(f'{x} new jobs updated in Jobs.csv.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954a3da",
   "metadata": {},
   "source": [
    "#### Now, we are going to filter the jobs data as required from the data scraped from the url and saved in the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295a4f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Data from\n",
      "https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=\n",
      ".\n",
      "3 new jobs updated in the Pythonjobs_filter.txt.\n"
     ]
    }
   ],
   "source": [
    "def findJobs():\n",
    "  count=0\n",
    "  html_text = requests.get(urls).text\n",
    "  soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "  jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "  unfamSkill = 'Django'  # Add your unfamiliar skill here\n",
    "\n",
    "# Saving the jobs filtered in text file\n",
    "\n",
    "  with open(f'posts/Pythonjobs_filter.txt','w') as f:  \n",
    "    for index, job in enumerate(jobs):\n",
    "      skills = job.find('ul', class_=\"list-job-dtl clearfix\").span.text\n",
    "      if unfamSkill.lower() not in skills.lower():  # Filter jobs that require the unfamiliar skill\n",
    "        location = job.find('ul', class_=\"top-jd-dtl clearfix\").span.text\n",
    "        company = job.find('h3', class_=\"joblist-comp-name\").text\n",
    "        link = job.find('header', class_=\"clearfix\").h2.a['href']\n",
    "        \n",
    "        count+=1\n",
    "        f.write(f\"Company: {company.strip().replace('(More Jobs)','')}\\n\")\n",
    "        f.write(f\"Location: {location.strip()}\\n\")\n",
    "        f.write(f\"Skills: {skills.strip()}\\n\")\n",
    "        f.write(f\"Link: {link}\\n\\n\")\n",
    "  \n",
    "  return count\n",
    "\n",
    "print(f'Fetching Data from\\n{urls}\\n.')\n",
    "x = findJobs()\n",
    "print(f'{x} new jobs updated in the Pythonjobs_filter.txt.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24908b",
   "metadata": {},
   "source": [
    "#### Finding jobs for a desired location and saving them in a new csv file named filterjobs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae799ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Data from\n",
      "https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=\n",
      ".\n",
      "3 new jobs found and updated in the filterjobs.csv file.\n"
     ]
    }
   ],
   "source": [
    "def findJobs():\n",
    "    count=0\n",
    "    html_text = requests.get(urls).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "    jobs_data = []  # List to store job data\n",
    "    desired_location = 'Noida'  # Replace with your desired location\n",
    "\n",
    "    for index, job in enumerate(jobs):\n",
    "        location = job.find('ul', class_=\"top-jd-dtl clearfix\").span.text\n",
    "        if desired_location.lower() in location.lower():  # Filter jobs by location\n",
    "            skills = job.find('ul', class_=\"list-job-dtl clearfix\").span.text\n",
    "            company = job.find('h3', class_=\"joblist-comp-name\").text\n",
    "            link = job.find('header', class_=\"clearfix\").h2.a['href']\n",
    "            \n",
    "            count+=1\n",
    "            jobs_data.append({\n",
    "                'Company': company.strip().replace('(More Jobs)',''),\n",
    "                'Location': location.strip(),\n",
    "                'Skills': skills.strip(),\n",
    "                'Link': link\n",
    "            })\n",
    "\n",
    "    # Save job data to a new CSV file\n",
    "    \n",
    "    if not os.path.exists('posts'):\n",
    "        os.makedirs('posts')\n",
    "    df = pd.DataFrame(jobs_data)\n",
    "    df.to_csv('posts/filterjobs.csv', index=False)\n",
    "\n",
    "    return count\n",
    "\n",
    "print(f'Fetching Data from\\n{urls}\\n.')\n",
    "x = findJobs()\n",
    "print(f'{x} new jobs found and updated in the filterjobs.csv file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5330a",
   "metadata": {},
   "source": [
    "##### Some more filteration can be done on the data extracted from the website, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb922551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Data from\n",
      "https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=\n",
      ".\n",
      "0 new jobs found as per desired filter.\n"
     ]
    }
   ],
   "source": [
    "def findJobs():\n",
    "    count=0\n",
    "    html_text = requests.get(urls).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "    jobs_data = []  # List to store job data\n",
    "    desired_job_type = 'Full Time'  # Replace with your desired job type\n",
    "\n",
    "    for index, job in enumerate(jobs):\n",
    "        location = job.find('ul', class_=\"top-jd-dtl clearfix\").span.text\n",
    "        job_type_elem = job.find('span', class_=\"type\")  # Replace with actual class name\n",
    "        if job_type_elem is not None:\n",
    "            job_type = job_type_elem.text\n",
    "            if desired_job_type.lower() in job_type.lower():  # Filter jobs by job type\n",
    "                skills = job.find('ul', class_=\"list-job-dtl clearfix\").span.text\n",
    "                company = job.find('h3', class_=\"joblist-comp-name\").text\n",
    "                link = job.find('header', class_=\"clearfix\").h2.a['href']\n",
    "                \n",
    "                count+=1\n",
    "                jobs_data.append({\n",
    "                    'Company': company.strip().replace('(More Jobs)',''),\n",
    "                    'Location': location.strip(),\n",
    "                    'Skills': skills.strip(),\n",
    "                    'Link': link\n",
    "                })\n",
    "\n",
    "    # Save job data to another CSV file\n",
    "    \n",
    "    if not os.path.exists('posts'):\n",
    "        os.makedirs('posts')\n",
    "    df = pd.DataFrame(jobs_data)\n",
    "    df.to_csv('posts/Desiredjobs.csv', index=False)\n",
    "\n",
    "    return count\n",
    "\n",
    "print(f'Fetching Data from\\n{urls}\\n.')\n",
    "x = findJobs()\n",
    "print(f'{x} new jobs found as per desired filter.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
